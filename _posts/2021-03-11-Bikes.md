---
layout: post
title:  "Part One - Autonomous Vehicles and Bicycles"
date:   2021-02-27 11:45:00
categories: jekyll css
---

{% newthought 'Autonomous vehicle technology is projected to revolutionize the transportation industry - but do we know how they interact with bicyclists?' %} 
{% maincolumn 'assets/img/Bicycles_AVs/bicycle-car-sensor-technology.jpg'  ''%}

<!--more-->

**My Friend Tom**

A roommate of mine, Tom, came home one day with a bandaged hand, bruised face, and a general look of shock and bewilderment. He had just come from the hospital after hitting a car on his bike. A negligent happenstance for some, but a truly unlucky feat for Tom.

Here’s what he told me: 

While riding into town, post-COVID shutdowns, the streets were empty. This was around the start of quarantine, early April 2020 - hardly anyone was out. Nonetheless, Tom encountered one vehicle on his way.

After shooting down a short, but steep hill, Tom came up on a mid-sized SUV. He was still coasting from the downhill and hoped to use this momentum for an easier ride on the subsequent flat part coming into town. 

Tom was accelerating so fast from the momentum, he started making moves to pass the car ahead of him. As he was coming up on the car’s back right bumper, but still a reasonable distance behind, a pedestrian streaked across the road ahead of the car. As Tom put it, the person appeared out of nowhere, whooped and sprinted through the crosswalk, then disappeared into a nearby park. And though the car was still a good distance from the crosswalk, it executed a full stop, the back wheels swinging into the bike lane. 

Tom, still trying to maximize his speed from the downhill, ran arm-first into the back window, while his face swung around and hit the side of the car. His hand, probably in trying to brace himself, punched through the bottom right corner of the back window and his arm went through all the way up to the shoulder. 

While this may seem like a clear case of an over-reactive driver, the driver, in fact, was not at fault. Instead, the car, a fresh-off-the-lot 2021 vehicle installed with an automatic emergency braking system (AEB), had made the decision to stop. Even though the car was far enough from the pedestrian for the driver to consciously reduce his speed, and even though the whooper/sprinter was speeding so fast through the crosswalk that the car wouldn’t have hit him regardless (according to Tom), the car had recognized a potential target and literally skidded to a halt. 

Both Tom and the driver paid for it, the driver in repairing his new car, and Tom with his medical bills and newly cracked bike frame. 

**So, what does this mean?**

I believe there are several takeaways from this story, perhaps the least of which is the question of when it’s prudent for a cyclist to rely on coasting. Tom had hoped to use the momentum from the downhill to ease his pedaling; I think most experienced cyclists have leant on passive energy for speed and ease of riding (plus, there’s a special kind of satisfaction derived from passing a car on your bike, especially while half-coasting). His determination to go faster, however, brought him in the line of fire. Which brings up another question: Is it right to pass another vehicle in such proximity when they are the only other vehicle on the roadway? Legality aside, is doing so inconsiderate if not rash? In retrospect, Tom acknowledges that his attempt at overtaking may have been reckless. Given the flatness of the road, Tom’s experience as a cyclist, and the lightness of his bike, he had no need to save himself the pedaling effort nor outpace the only other vehicle on the roadway. 

But the question I ultimately believe to be most poignant here is that of how autonomous vehicles (AVs) will coalesce with bicyclists in the future. I don’t believe one could truly attribute Tom's accident to the AEBs, given that they were attempting to save the driver and pedestrian from needless harm. However, the fact that an accident resulted nonetheless suggests a need for further analysis of how autonomous vehicles in general interact with bicyclists, as they may not prioritize bicyclist's safety as highly as they should.

**But first – A Little Background**

What technology allows autonomous vehicles to be autonomous? And is it fair to question AVs in general despite Tom’s story only featuring AEBs?

In short, autonomous here means driver-less, or else, having little to no need for human supervision. But within this, AVs can be deconstructed into ascending levels of autonomy. The Society of Automotive Engineers (SAE) categorized AVs in 2013, and then [updated them again](https://www.sae.org/news/press-room/2018/12/sae-international-releases-updated-visual-chart-for-its-"levels-of-driving-automation"-standard-for-self-driving-vehicles) in 2018, developing 6 tiers of automation, from Level 0, in which the vehicle only makes suggestions to a human driver and occasionally takes over (as evident in Tom's braking case), to Level 5, where the vehicle has complete control and passengers have confidence in not supervising. Between Levels 2 to 3, is the threshold between the human’s and vehicle's control; in other words, Level 3 signals the beginning of vehicle autonomy. Despite some company’s misleading advertising, like Tesla’s “[full self-driving](https://www.thedrive.com/news/26700/tesla-puts-full-self-driving-back-on-the-menu-but-its-not-what-you-think)” tech package, as yet, no company involved with AV technology has reached level 5. The table below details each level. 

{% maincolumn 'assets/img/Bicycles_AVs/AVs level of Autonomy 3.jpg' "Before the update in 2018 only 5 levels existed, 0-4. As technology progressed, Level 4 developed a part (a) and (b). Part (b) was eventually bisected into a separate level - Level 5. Images detailing AV tech, history, and projections refer to fully autonomous as Level 4 or Level 5 depending on when they were created." %} 

Looking back at Tom’s story, we don’t see a Level 5 driverless vehicle. Instead, his experience features a human-driven car equipped with AEBs, a form of brake assist designed to help the driver identify and avoid head on collisions, putting this vehicle at a Level 0/Level 1, according to SAE’s interpretation. {% sidenote '1' "It’s also worth noting, that since SAE created the autonomy levels, they have taken on a life of their own. It’s unclear if SAE sanctioned any of the changes or if they were simplified via the proverbial ‘telephone’ process. Many other sources would not characterize AEBs at Level 1 at the least."%} 

{% maincolumn 'assets/img/Bicycles_AVs/AEB.jpg' "" %} These brakes become operational with a command from a computer that receives information from built-in sensors. When these sensors detect possible impending obstacles, they relay that data to the computer, which decides if they are, indeed, obstacles to be avoided; if so, the computer gives the word to brake. In other words, AEBs are a low-level form of artificial intelligence (AI). Like AEB’s, AI makes decisions according to information it receives. While the AI of today is getting increasingly more complicated, analyzing a broader spectrum of data in more depth and outputting information akin to a human’s intelligence (and in many ways faster), at their core they operate by gathering, synthesizing, and processing data. Principally, this process is the same as an AEB's decision making, and therefore, an AV's decision making as well.

Thus, despite Tom’s brush with what could be considered a limited version of AI, these brakes are in fact the immature fledglings of a fully autonomous vehicle. More than that, the AVs of today are intended by some to be the prototype of a nationwide fleet of vehicles.  [Projections from the Governors Highway Safety Association](https://www.ghsa.org/resources/spotlight-av17) (GHSA) predict anywhere from 20 - 40% of vehicles manufactured by 2040 will be autonomous. 

{% fullwidth 'assets/img/Bicycles_AVs/TIMELINE-TO-FULL-CAR-AUTOMATION.png' ''%}

Already, the technology that enables AVs has grown exponentially in the last decade, to the point where we currently see functioning Level 4 vehicles on roadways worldwide (albeit on a trial level as most are still under supervision, and prone to adjustments as the systems run into novel situations). And progress begets progress, as they say. Given this projected eventuality, I believe it’s fair to interrogate the nature of AVs in general, as they are poised to be the next greatest technological advancement since the bread slicer gave us sliced bread.

Driverless-ness, in general, however, is a tall order. While certainly more than the sum of its parts, when disassembled, we can see the complex systems that AVs must employ to function. Among technologies like GPS and automatic braking and steering, Alan Amici, vice president of TE Connectivity, also [identifies the 3 most essential AV elements](https://www.fi.edu/science-of-selfdriving-cars): sensors, connectivity, and software/control algorithms.



*Sensors*

Sensors are pivotal to the car’s ability to navigate and more than that, they do the data groundwork, feeding information to the other systems to build from. Sensors must observe their surroundings, detect objects on all sides, identify lane boundaries or curves in the road, etc. We see these sensors in the form of 3 basic technologies working simultaneously: 360-degree LIDAR and radar for short and long range object detection{% sidenote '3' '[This article](https://www.thedrive.com/article/16916/lidar-vs-radar-pros-and-cons-of-different-autonomous-driving-technologies) discusses how LIDAR and radar work, their differences, pros and cons, and which of the two sensors individual AV companies prefer' %}, cameras for cross-checking object recognition, and ultrasound for park assist. 

These provide the AVs with a visual field comparable to the human eye, allowing them to “see” as a human would and orient themselves in relation to the objects around them. While sensors, at the moment, may not out-compete a human eye, several benefits of this technology are that sensors are constantly vigilant, consistent in their estimations, and attending to all sides of the vehicle at once. 

{% maincolumn 'assets/img/Bicycles_AVs/Sensors.png' "Each individual sensor's visual field." %} 



*Algorithms*

All this sensory data, however, is useless to a system that can’t make confident, human-esque decisions; it needs an on-deck computer system to compile information and learn from the data accumulated until it has developed a situationally-aware, and in some ways, superhuman system {% sidenote '3' 'I won’t go into the different kinds of AI models employed by AV’s, however [this article](https://www.jdsupra.com/legalnews/the-basics-of-autonomous-vehicles-part-35016/) details many of the commonly used algorithms.'%}. In other words, it needs AI. And like a teenager with a learner’s permit, the system garners skill through experience. In doing so, the algorithm learns to discriminate between ambiguous objects like a plastic bag and a child, a shady patch and a pothole, or highway construction and a cyclist. 

For the most part, AVs use neural networks, a machine learning tool modeled off the human brain. The networks, once fed a sufficient amount of images, let's say of a bicycle (think of the copious amount of images parlayed through the captcha assignments to prove humanity), will eventually ascertain for itself what a bicycle is without being told. The images prime the model in a training setting, so it understands what it is looking at in the 'real' world. Once it identifies a bicyclist as such, the model can then make a decision. Its next move may be to consciously give the bicyclist 3 feet of breathing room while passing. Among the 50+ companies involved with autonomous vehicles, there are dozens of models available. I’ll discuss one company's model in more detail later.

{% maincolumn 'assets/img/Bicycles_AVs/How AVs work.jpg' "This image from Nissan summarizes the algorithmic process succinctly." %} 

Not only do these models need to identify *what* they are looking at, they are also need to reconcile how fast and in what direction surrounding objects are moving. And in doing so, an algorithm should be able to predict the movements of these objects to inform its own navigational decisions. For instance, if the vehicle wants to switch lanes, it must determine if any entity is currently occupying the space it seeks to fill, or if there is an approaching vehicle about to pass. If so, it must calculate that vehicle’s speed and adjust accordingly. Alternatively, if a deer runs across the road, while perhaps unpredictable, it would be useful to know if that deer would already be out of the lane by the time the vehicle gets there. Thus, in calculating the deer’s speed and direction (or in other words, its velocity) the AI may determine if it only needs to slow down instead of stop, perhaps saving its passengers unnecessary stress.



*Connectivity*

Lastly, while the AVs are fully operational with just sensors and algorithms, they also need input from surrounding vehicles or wireless hubs (i.e. connectivity) if they are to be more efficient and operate in proximity. These connections could provide weather and traffic updates, road closure information, and input from surrounding AVs. 

The picture below summarizes the big 5 in AV wireless tech.

The 5 Points of Connectivity:

{% maincolumn 'assets/img/Bicycles_AVs/connectivity areas.png' "" %} 

* V2P _(Vehicle to Pedestrian)_ - this could refer either to linking with a pedestrians phone to locate them in relation to the vehicle or linking to another device purposefully worn by the pedestrian to allow the vehicle to locate them
- V2V _(Vehicle to Vehicle)_ - this refers to the picture above, in which all AVs in proximity are connected and communicated to avoid dangerous situations or make transportation more efficient
* V2I _(Vehicle to Infrastructure)_ - as this image note, V2I allows the vehicles to connect with local stoplights and the like to let them know the vehicle is there. For instance, if stopped at a stoplight, with no other vehicles around, the AV could communicate its position, and turn the light green.
* V2D _(Vehicle to Device)_ - As the image says, this kind of connection would hook in with local devices to gather information, like images from a camera detailing a traffic accident, or sensors around intersections beeping for a pedestrian walking. This could include pedestrian smart phones as well. 
* V2X _(Vehicle to Everything)_ -  This encompasses everything else that doesn't fall under the other headers. This includes individual objects such as trees, buildings, rocks and the like, which may have sensors attached to them to inform AVs of their location. This could be helpful in circumnavigating unique roadway situations where a tree, for instance, is situated very closely to the road. The vehicle may mistake it for a pedestrian or cyclist, of which, it would need to provide extra space for.

V2V communication is especially important in avoiding dangerous situations between AVs. If a vehicle decides to stop because it thinks the plastic bag is a child, it needs to provide a forewarning to the next vehicle that it’s stopping, so the next car may simultaneously and safely stop or else provide a wide berth. Additionally, fleets of AVs on the roadway, if not interconnected, may be dangerous, if not simply inefficient. Take for example two AVs arriving at an intersection at the exact same time. While AEBs may prevent them from running into each other, without the ability to communicate, the AVs may become locked in an interminable stalemate as they attempt to navigate past each other. That may only occur, however, if they aren’t able to communicate their intentions and synchronize their movements. Thus, AVs cannot operate safely on independent systems. Check out the image below for more examples where connected AVs can avoid tricky situations.

{% maincolumn 'assets/img/Bicycles_AVs/V2V.png' "" %} 

Beyond safety, connectivity could also boost efficiency overall. Traffic could synchronize  {% sidenote '4' 'Check out [this video](https://www.youtube.com/watch?v=iHzzSao6ypE&t=253s) on how phantom traffic forms and [this video](https://www.youtube.com/watch?v=TNokBgtSUvQ&t=255s) for a more detailed look. While I prefer the first video because it provides more information in less time, the animation in the second video is hard to beat.'%}, eliminating the-mile-long start/stop jerking; stop lights could become non-existent, intersections in general would need little oversight. In a large intersection, vehicles would automatically make space for each other. And, an overarching algorithmic system, essentially acting as an air traffic controller, could calculate each vehicle’s route, determine where they will be at any given moment, and ultimately, minimize everyone’s time in traffic. Vehicles could also receive moment by moment reports from traffic and accident centers, like The Department of Transportation, local newscasters, or Google Maps. This could be a boon for a vehicle’s passengers; AVs could automatically switch routes due to road closures, traffic, or iciness, saving passengers time and reducing contact with safety hazards. 

*Research and Regulation*

Collectively, these technologies form a functional efficient system that, with enough sensors, positioned correctly, combined with reliable programming and wireless information boosts, could be more preferable than conventional vehicles. Despite their potential, however, AV technology is developing faster than the policies to regulate them. Without firm regulatory guidelines, not all of the AVs on the market today are put through the same safety procedures with the same level of testing and algorithmic standards. Both Tesla and Uber have infamously mis-advertised or cut corners on their products. With all this in mind, I'll do a shallow dive into AV research in part 2.



**Autonomous Vehicle events**

While not necessary for this article, it's also interesting to see how far autonomous vehicle technology has come, and more than that, how quickly. I'm sure visions of self-driving vehicles existed previous to these milestones, but officially, as a population, we have been imagining an automated world for almost one hundred years.

{% maincolumn 'assets/img/Bicycles_AVs/AVs timeline.jpg' 'This image outlines important milestones in automation history, however, it misses a few key pitstops. 1.) In 1939, Norman Bel Geddes created the [Futurama exhibit](https://www.sensesatlas.com/territory/architecture/futurama-the-prototype-of-the-american-highway-city/) in New York’s 1939 World’s Fair, sponsored by General Motors, and featuring an automated highway. The exhibit imagined the world 20 years in the future and was the largest scale model in history. 2.) More importantly to current technological advancements, in 1986, German engineer [Ernst Dickmann](https://www.politico.eu/article/delf-driving-car-born-1986-ernst-dickmanns-mercedes/) outfitted a Mercedes-Benz van with autonomous technology and drove it around his university campus. He then began a partnership with German car manufacturer Daimler; Dickmann’s vision is just recently coming to fruition as social experimentation and technology are converging. 3.) This image mentions the creation of the [DARPA Grand Challenge](https://www.darpa.mil/work-with-us/public/prizes) in 2004, funded by the Defense Advanced Research Projects Agency, a branch of the US Department of Defense. What this image does not mention, however, is that the first two challenges were long distance races located in the Mojave Desert with a 1 and then 2-million-dollar prize. The third challenge, in 2007, was an Urban course, located at an Air Force Base in California. Prizes were again $2 million dollars. DARPA has since continued with cyber, space launch, and subterranean challenges. 4.) In 2015, the [Delphi Automotive](https://www.autoblog.com/2015/04/03/delphi-thrilled-with-results-from-autonomous-cars-cross-country/) driverless car becomes the first AV to complete a transcontinental trip from San Francisco to NYC. It completed it in 9 days. 5.) And in 2016, Singapore introduced its first autonomous taxi service, [MIT’s nuTonomy](https://spectrum.ieee.org/cars-that-think/transportation/self-driving/nutonomy-to-launch-worlds-first-fully-autonomous-taxi-service-in-singapore-this-year), to its streets.'%} 
